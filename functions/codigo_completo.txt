# Arquivo: query_monitor.py

## query_monitor

import logging
import hashlib
from typing import List, Dict, Any
from functions.db_utils import OracleTableReader
from functions.utils import SQLParser

logger = logging.getLogger(__name__)

def identify_fts_queries(db_reader: OracleTableReader) -> List[Dict[str, Any]]:
    """
    Identifica queries FTS (Full Table Scan) no banco de dados Oracle.
    
    Args:
        db_reader: InstÃ¢ncia de OracleTableReader para conexÃ£o com o banco
        
    Returns:
        Lista de dicionÃ¡rios contendo informaÃ§Ãµes das queries FTS encontradas
    """
    query = """
        SELECT sql_id, sql_text, executions, elapsed_time
        FROM v$sql
        WHERE sql_text LIKE '%TABLE ACCESS FULL%' 
           OR sql_text LIKE '%/*+ FULL(%'
        ORDER BY elapsed_time DESC
    """
    
    try:
        results = db_reader.execute_query(query)
        if not results:
            logger.info("Nenhuma query FTS encontrada.")
            return []
        
        queries = []
        for row in results:
            sql_id, sql_text, executions, elapsed_time = row
            
            try:
                tables = SQLParser.extract_tables(sql_text)
                where_conditions = SQLParser.extract_where_conditions(sql_text)
                schema = db_reader.get_table_schema(tables[0]) if tables else None
                
                queries.append({
                    "sql_id": sql_id,
                    "sql_text": sql_text,
                    "executions": executions,
                    "elapsed_time": elapsed_time,
                    "tables": tables,
                    "where_conditions": where_conditions,
                    "schema": schema
                })
                
            except Exception as e:
                logger.warning(f"Erro ao processar query {sql_id}: {str(e)}")
                continue
        
        logger.info(f"Identificadas {len(queries)} queries FTS")
        return queries
        
    except Exception as e:
        logger.error(f"Falha ao buscar queries FTS: {str(e)}", exc_info=True)
        return []

def group_similar_queries(queries: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Agrupa queries similares com base em tabelas e condiÃ§Ãµes WHERE.
    
    Args:
        queries: Lista de queries FTS a serem agrupadas
        
    Returns:
        Lista de grupos de queries com estatÃ­sticas consolidadas
    """
    groups = {}
    
    for query in queries:
        # Criar assinatura Ãºnica para o grupo
        signature = _create_query_signature(
            query.get("tables", []),
            query.get("where_conditions", [])
        )
        
        # Inicializar grupo se nÃ£o existir
        if signature not in groups:
            groups[signature] = _initialize_query_group(query, signature)
        
        # Atualizar estatÃ­sticas do grupo
        _update_group_stats(groups[signature], query)
    
    # Calcular mÃ©tricas finais e prioridades
    grouped_queries = list(groups.values())
    for group in grouped_queries:
        group["avg_exec_time"] = group["total_exec_time"] / group["count"]
        group["priority_score"] = _calculate_priority_score(group)
    
    logger.info(f"Criados {len(grouped_queries)} grupos de queries")
    return sorted(grouped_queries, key=lambda x: x["priority_score"], reverse=True)

def _create_query_signature(tables: List[str], conditions: List[str]) -> str:
    """Cria assinatura Ãºnica para agrupamento de queries"""
    normalized = f"{sorted(tables)} WHERE {sorted(conditions)}"
    return hashlib.md5(normalized.encode()).hexdigest()

def _initialize_query_group(query: Dict[str, Any], signature: str) -> Dict[str, Any]:
    """Inicializa um novo grupo de queries"""
    return {
        "group_key": signature,
        "sample_sql": query["sql_text"],
        "normalized_key": f"{query['tables']} WHERE {query['where_conditions']}",
        "count": 0,
        "total_exec_time": 0,
        "tables": list(set(query.get("tables", []))),
        "schemas": set(),
        "where_conditions": list(set(query.get("where_conditions", []))),
        "priority_score": 0,
        "sql_ids": []
    }

def _update_group_stats(group: Dict[str, Any], query: Dict[str, Any]) -> None:
    """Atualiza estatÃ­sticas de um grupo com nova query"""
    group["count"] += 1
    group["total_exec_time"] += query.get("elapsed_time", 0)
    if query.get("schema"):
        group["schemas"].add(query["schema"])
    group["sql_ids"].append(query["sql_id"])

def _calculate_priority_score(group: Dict[str, Any]) -> float:
    """Calcula pontuaÃ§Ã£o de prioridade para o grupo"""
    exec_weight = group["count"] * 0.6
    time_weight = (group["total_exec_time"] / 1000) * 0.4  # Converter para ms
    return round(exec_weight + time_weight, 2)
# Teste: test_query_monitor.py

# test_query_monitor.py
import pytest
from unittest.mock import patch
from functions.query_monitor import (
    identify_fts_queries,
    group_similar_queries,
    _create_query_signature,
    _initialize_query_group,
    _update_group_stats,
    _calculate_priority_score
)

class MockOracleTableReader:
    def __init__(self):
        self.test_data = [
            ("sql1", "SELECT * FROM users WHERE id = 1", 10, 1000),
            ("sql2", "SELECT * FROM products WHERE price > 100", 5, 2000),
            ("sql3", "SELECT * FROM users WHERE name LIKE 'A%'", 8, 1500)
        ]
    
    def execute_query(self, query, params=None, fetch_all=True):
        return self.test_data
    
    def get_table_schema(self, table_name):
        return f"schema_{table_name.lower()}"

class MockSQLParser:
    @staticmethod
    def extract_tables(sql_text):
        if "FROM" not in sql_text:  # Para queries sem tabelas explÃ­citas
            return []
        if "users" in sql_text:
            return ["users"]
        elif "products" in sql_text:
            return ["products"]
        elif "DUAL" in sql_text:  # Para queries com FROM DUAL
            return []
        return []
    
    @staticmethod
    def extract_where_conditions(sql_text):
        if "WHERE" not in sql_text:
            return []
        if "id = 1" in sql_text:
            return ["id = 1"]
        elif "price > 100" in sql_text:
            return ["price > 100"]
        elif "name LIKE 'A%'" in sql_text:
            return ["name LIKE 'A%'"]
        return []

@pytest.fixture
def mock_db_reader():
    return MockOracleTableReader()

# -----------------------------------------------------------
# TESTES PRINCIPAIS
# -----------------------------------------------------------

def test_identify_fts_queries(mock_db_reader):
    """Testa identificaÃ§Ã£o bÃ¡sica de queries FTS"""
    with patch('functions.query_monitor.SQLParser', new=MockSQLParser):
        results = identify_fts_queries(mock_db_reader)
        
        assert len(results) == 3
        assert results[0]["sql_id"] == "sql1"
        assert results[1]["tables"] == ["products"]
        assert results[2]["where_conditions"] == ["name LIKE 'A%'"]
        assert results[0]["schema"] == "schema_users"

def test_group_similar_queries():
    """Testa agrupamento de queries similares"""
    test_queries = [
        {
            "sql_id": "sql1",
            "sql_text": "SELECT * FROM users WHERE id = 1",
            "executions": 10,
            "elapsed_time": 1000,
            "tables": ["users"],
            "where_conditions": ["id = 1"],
            "schema": "schema_users"
        },
        {
            "sql_id": "sql2",
            "sql_text": "SELECT * FROM users WHERE id = 2",
            "executions": 5,
            "elapsed_time": 2000,
            "tables": ["users"],
            "where_conditions": ["id = 2"],
            "schema": "schema_users"
        }
    ]
    
    groups = group_similar_queries(test_queries)
    assert len(groups) == 2
    assert groups[0]["count"] == 1
    assert groups[0]["tables"] == ["users"]

def test_query_signature():
    """Testa geraÃ§Ã£o de assinatura Ãºnica para queries"""
    sig1 = _create_query_signature(["users"], ["id = 1"])
    sig2 = _create_query_signature(["users"], ["id = 2"])
    sig3 = _create_query_signature(["products"], ["price > 100"])
    
    assert sig1 != sig2
    assert sig1 != sig3
    assert len(sig1) == 32  # MD5 hash length

def test_priority_score_calculation():
    """Testa cÃ¡lculo de score de prioridade"""
    group = {
        "count": 10,
        "total_exec_time": 5000  # 5 segundos
    }
    score = _calculate_priority_score(group)
    assert score == 8.0  # (10*0.6 + 5*0.4)

def test_empty_results(mock_db_reader):
    """Testa comportamento com resultados vazios"""
    mock_db_reader.test_data = []
    with patch('functions.query_monitor.SQLParser', new=MockSQLParser):
        results = identify_fts_queries(mock_db_reader)
        assert results == []

# -----------------------------------------------------------
# TESTES ADICIONAIS PARA CASOS ESPECIAIS
# -----------------------------------------------------------

def test_queries_with_no_tables(mock_db_reader):
    """Testa queries que nÃ£o referenciam tabelas"""
    mock_db_reader.test_data = [
        ("sql4", "SELECT 1 FROM DUAL", 1, 100),
        ("sql5", "SELECT SYSDATE", 1, 50)
    ]
    
    with patch('functions.query_monitor.SQLParser', new=MockSQLParser):
        results = identify_fts_queries(mock_db_reader)
        assert len(results) == 2
        assert results[0]["tables"] == []
        assert results[1]["tables"] == []

def test_malformed_queries(mock_db_reader):
    """Testa tratamento de SQL invÃ¡lido"""
    mock_db_reader.test_data = [
        ("sql6", "INVALID SQL SYNTAX", 1, 100),
        ("sql7", "SELECT * FROM", 1, 150)  # SQL incompleto
    ]
    
    with patch('functions.query_monitor.SQLParser', new=MockSQLParser):
        results = identify_fts_queries(mock_db_reader)
        assert len(results) == 2
        assert results[0]["tables"] == []
        assert results[1]["tables"] == []

def test_queries_with_complex_conditions(mock_db_reader):
    """Testa queries com mÃºltiplas condiÃ§Ãµes WHERE"""
    mock_db_reader.test_data = [
        ("sql8", "SELECT * FROM orders WHERE status='OPEN' AND value>1000", 3, 750)
    ]
    
    with patch('functions.query_monitor.SQLParser') as mock_parser:
        mock_parser.extract_tables.return_value = ["orders"]
        mock_parser.extract_where_conditions.return_value = ["status='OPEN'", "value>1000"]
        
        results = identify_fts_queries(mock_db_reader)
        assert len(results[0]["where_conditions"]) == 2

# -----------------------------------------------------------
# TESTES DE PERFORMANCE (OPCIONAIS)
# -----------------------------------------------------------

@pytest.mark.benchmark
def test_performance_with_large_dataset(benchmark, mock_db_reader):
    """Teste de performance com grande volume de queries"""
    mock_db_reader.test_data = [
        (f"sql{i}", f"SELECT * FROM table{i%10} WHERE col={i}", i%10, i*100)
        for i in range(1000)  # 1000 queries simuladas
    ]
    
    with patch('functions.query_monitor.SQLParser', new=MockSQLParser):
        result = benchmark(identify_fts_queries, mock_db_reader)
        assert len(result) == 1000


# Arquivo: table_analysis.py
import logging
from typing import Dict, List, Any, Optional, TypedDict
from functions.db_utils import OracleTableReader

logger = logging.getLogger(__name__)

class TableInfo(TypedDict):
    """Typed dictionary for table information"""
    table: str
    size_mb: float
    schema: Optional[str]
    indexes: Optional[List[str]]

class ClassificationResult(TypedDict):
    """Typed dictionary for classification results"""
    T1: List[TableInfo]
    T2: List[TableInfo]

def classify_tables(
    db_reader: OracleTableReader, 
    queries: List[Dict[str, Any]],
    size_threshold: float = 10.0
) -> ClassificationResult:
    """
    Classify tables into T1 (< threshold) and T2 (>= threshold) based on size
    
    Args:
        db_reader: OracleTableReader instance
        queries: List of query dictionaries
        size_threshold: Size threshold in MB (default: 10.0)
        
    Returns:
        Dictionary with tables classified into T1 and T2
    """
    # Input validation
    if not isinstance(db_reader, OracleTableReader):
        logger.error("db_reader must be an OracleTableReader instance")
        raise TypeError("db_reader must be an OracleTableReader instance")
        
    if not isinstance(queries, list):
        logger.error("queries must be a list")
        raise TypeError("queries must be a list")

    # Initialize result structure
    result: ClassificationResult = {
        "T1": [],
        "T2": []
    }
    
    # Get unique tables from all queries
    tables = sorted({table.upper() for query in queries 
                    if isinstance(query, dict)
                    for table in query.get("tables", [])
                    if isinstance(table, str) and table.strip()})
    
    if not tables:
        logger.info("No tables found in queries")
        return result
    
    # Process each table
    for table in sorted(tables):  # Process in consistent order
        try:
            size = db_reader.get_table_size(table)
            if size is None:
                logger.warning(f"Could not get size for table {table}")
                continue
                
            schema = db_reader.get_table_schema(table)
            indexes = db_reader.get_existing_indexes(table)
            
            table_info: TableInfo = {
                "table": table,
                "size_mb": size,
                "schema": schema,
                "indexes": indexes if indexes else None
            }
            
            if size < size_threshold:
                result["T1"].append(table_info)
                logger.debug(f"Table {table} classified as T1 ({size:.2f} MB)")
            else:
                result["T2"].append(table_info)
                logger.debug(f"Table {table} classified as T2 ({size:.2f} MB)")
                
        except Exception as e:
            logger.error(f"Error processing table {table}: {str(e)}")
            continue
    
    logger.info(
        f"Classification complete - T1: {len(result['T1'])} tables | "
        f"T2: {len(result['T2'])} tables | "
        f"Threshold: {size_threshold} MB"
    )
    
    return result


def analyze_table_access_patterns(
    db_reader: OracleTableReader,
    queries: List[Dict[str, Any]],
    classification: ClassificationResult
) -> Dict[str, Any]:
    """
    Analyze table access patterns based on query usage
    
    Args:
        db_reader: OracleTableReader instance
        queries: List of analyzed queries
        classification: Classification result
        
    Returns:
        Dictionary with access pattern statistics
    """
    access_stats = {
        "high_usage_tables": [],
        "join_patterns": {},
        "filter_conditions": {}
    }
    
    # Get all classified tables in uppercase
    all_tables = [t["table"].upper() for t in classification["T1"] + classification["T2"]]
    
    for query in queries:
        if not isinstance(query, dict):
            continue
            
        query_tables = query.get("tables", [])
        if not isinstance(query_tables, list):
            continue
            
        for table in query_tables:
            table_upper = table.upper()
            if table_upper in all_tables:
                if table_upper not in access_stats["join_patterns"]:
                    access_stats["join_patterns"][table_upper] = 0
                access_stats["join_patterns"][table_upper] += 1
    
    return access_stats


def generate_optimization_recommendations(
    classification: ClassificationResult,
    access_stats: Dict[str, Any]
) -> List[str]:
    """
    Generate optimization recommendations based on classification and access patterns
    
    Args:
        classification: Classification result
        access_stats: Access pattern statistics
        
    Returns:
        List of optimization recommendations
    """
    recommendations = []
    
    # Recommendations for T1 tables
    for table in classification["T1"]:
        rec = f"Table {table['table']} (T1 - {table['size_mb']:.2f}MB): "
        
        if not table['indexes']:
            rec += "Consider adding basic indexes"
        elif len(table['indexes']) < 3:
            rec += "Review existing indexes for optimization"
        else:
            rec += "Has sufficient indexes"
            
        recommendations.append(rec)
    
    # Recommendations for T2 tables
    for table in classification["T2"]:
        rec = f"Table {table['table']} (T2 - {table['size_mb']:.2f}MB): "
        
        if table['size_mb'] > 100:
            rec += "Consider partitioning "
        else:
            rec += "Evaluate future partitioning strategies "
            
        join_count = access_stats["join_patterns"].get(table['table'].upper(), 0)
        if join_count > 5:
            rec += "| Frequently joined - optimize relationship indexes"
            
        recommendations.append(rec)
    
    return recommendations
# Teste: test_table_analysis.py

import pytest
from unittest.mock import MagicMock, patch
from functions.table_analysis import (
    classify_tables,
    analyze_table_access_patterns,
    generate_optimization_recommendations,
    TableInfo,
    ClassificationResult
)
from functions.db_utils import OracleTableReader

class TestTableAnalysis:
    @pytest.fixture
    def mock_reader(self):
        """Fixture providing a mocked OracleTableReader"""
        mock = MagicMock(spec=OracleTableReader)
        mock._last_error = None
        return mock

    @pytest.fixture
    def sample_queries(self):
        """Fixture with sample queries for testing"""
        return [
            {"tables": ["users"], "sql": "SELECT * FROM users"},
            {"tables": ["orders"], "sql": "SELECT * FROM orders WHERE status = 'shipped'"},
            {"tables": ["customers", "orders"], "sql": "JOIN query"}
        ]

    @pytest.fixture
    def sample_classification(self):
        """Fixture with sample classification results"""
        return {
            "T1": [
                {"table": "USERS", "size_mb": 5.0, "schema": "APP", "indexes": ["users_pk"]}
            ],
            "T2": [
                {"table": "ORDERS", "size_mb": 15.0, "schema": "APP", "indexes": ["orders_pk"]},
                {"table": "CUSTOMERS", "size_mb": 25.0, "schema": "APP", "indexes": None}
            ]
        }

    def test_classify_tables_basic(self, mock_reader, sample_queries):
        """Basic table classification test"""
        # Configure mock responses in explicit order
        mock_responses = {
            "USERS": (5.0, "APP", ["users_pk"]),
            "ORDERS": (15.0, "APP", ["orders_pk"]),
            "CUSTOMERS": (25.0, "APP", None)
        }
        
        def get_table_size(table):
            return mock_responses[table.upper()][0]
            
        def get_table_schema(table):
            return mock_responses[table.upper()][1]
            
        def get_existing_indexes(table):
            return mock_responses[table.upper()][2]
        
        mock_reader.get_table_size.side_effect = get_table_size
        mock_reader.get_table_schema.side_effect = get_table_schema
        mock_reader.get_existing_indexes.side_effect = get_existing_indexes
        
        result = classify_tables(mock_reader, sample_queries)
        
        # Verify counts
        assert len(result["T1"]) == 1
        assert len(result["T2"]) == 2
        
        # Verify specific tables
        t1_tables = [t["table"] for t in result["T1"]]
        t2_tables = [t["table"] for t in result["T2"]]
        
        assert "USERS" in t1_tables
        assert "ORDERS" in t2_tables
        assert "CUSTOMERS" in t2_tables
        
        # Verify metadata
        users_info = next(t for t in result["T1"] if t["table"] == "USERS")
        assert users_info["size_mb"] == 5.0
        assert users_info["schema"] == "APP"
        assert users_info["indexes"] == ["users_pk"]

    def test_classify_tables_empty_input(self, mock_reader):
        """Test with empty query list"""
        result = classify_tables(mock_reader, [])
        assert result == {"T1": [], "T2": []}

    def test_classify_tables_invalid_input(self, mock_reader):
        """Test with invalid inputs"""
        with pytest.raises(TypeError):
            classify_tables(None, [])  # Invalid db_reader
            
        with pytest.raises(TypeError):
            classify_tables(mock_reader, "not_a_list")  # Invalid queries

    def test_classify_tables_with_none_size(self, mock_reader):
        """Test when table size is unavailable"""
        mock_reader.get_table_size.return_value = None
        queries = [{"tables": ["unknown_table"]}]
        
        result = classify_tables(mock_reader, queries)
        assert result == {"T1": [], "T2": []}

    def test_classify_tables_duplicates(self, mock_reader):
        """Test with duplicate tables in queries"""
        mock_reader.get_table_size.return_value = 8.0
        mock_reader.get_table_schema.return_value = "APP"
        mock_reader.get_existing_indexes.return_value = ["pk"]
        
        queries = [
            {"tables": ["users"]},
            {"tables": ["users", "orders"]},
            {"tables": ["users"]}
        ]
        
        result = classify_tables(mock_reader, queries)
        assert len(result["T1"]) == 2  # users and orders (only one entry for users)

    def test_classify_tables_custom_threshold(self, mock_reader, sample_queries):
        """Test with custom size threshold"""
        mock_reader.get_table_size.side_effect = [15.0, 25.0, 5.0]
        mock_reader.get_table_schema.return_value = "APP"
        
        # 20MB threshold
        result = classify_tables(mock_reader, sample_queries, size_threshold=20.0)
        
        assert len(result["T1"]) == 2  # orders (15) and users (5)
        assert len(result["T2"]) == 1  # customers (25)

    def test_classify_tables_mixed_query_formats(self, mock_reader):
        """Test with varied query formats"""
        mock_reader.get_table_size.side_effect = [5.0, 15.0, 8.0, None]
        mock_reader.get_table_schema.return_value = "APP"
        mock_reader.get_existing_indexes.return_value = ["pk"]
        
        queries = [
            {"tables": ["valid1"]},
            {"tables": "not_a_list"},  # invalid
            {"tables": ["valid2"]},
            {},  # no tables field
            {"tables": ["valid3"]},
            {"tables": [None, ""]}  # invalid
        ]
        
        result = classify_tables(mock_reader, queries)
        assert len(result["T1"]) == 2  # valid1 (5), valid3 (8)
        assert len(result["T2"]) == 1  # valid2 (15)

    def test_analyze_table_access_patterns(self, sample_classification, sample_queries):
        """Test table access pattern analysis"""
        mock_reader = MagicMock()
        result = analyze_table_access_patterns(mock_reader, sample_queries, sample_classification)
        
        assert "high_usage_tables" in result
        assert "join_patterns" in result
        assert isinstance(result["join_patterns"], dict)
        
        # Verify join counts
        assert result["join_patterns"].get("USERS") == 1
        assert result["join_patterns"].get("ORDERS") == 2  # appears in 2 queries
        assert result["join_patterns"].get("CUSTOMERS") == 1

    def test_generate_optimization_recommendations(self, sample_classification):
        """Test optimization recommendation generation"""
        access_stats = {
            "join_patterns": {
                "USERS": 5,
                "ORDERS": 10,
                "CUSTOMERS": 3
            }
        }
        
        recommendations = generate_optimization_recommendations(
            sample_classification,
            access_stats
        )
        
        assert len(recommendations) == 3  # 1 T1 + 2 T2
        assert any("T1" in rec for rec in recommendations)
        assert any("T2" in rec for rec in recommendations)
        
        # Verify recommendations contain expected info
        assert any("USERS" in rec for rec in recommendations)
        assert any("ORDERS" in rec for rec in recommendations)
        assert any("CUSTOMERS" in rec for rec in recommendations)
        assert any("partitioning" in rec.lower() for rec in recommendations)

    @patch('functions.table_analysis.logger')
    def test_classify_tables_logging(self, mock_logger, mock_reader, sample_queries):
        """Test logging messages"""
        mock_reader.get_table_size.side_effect = [5.0, 15.0, 25.0]
        mock_reader.get_table_schema.return_value = "APP"
        
        classify_tables(mock_reader, sample_queries)
        
        # Verify final log message
        mock_logger.info.assert_called_with(
            "Classification complete - T1: 1 tables | T2: 2 tables | Threshold: 10.0 MB"
        )

    def test_classify_tables_error_handling(self, mock_reader, sample_queries):
        """Test error handling during classification"""
        mock_reader.get_table_size.side_effect = [
            5.0,
            Exception("Simulated error"),
            25.0
        ]
        mock_reader.get_table_schema.return_value = "APP"
        
        result = classify_tables(mock_reader, sample_queries)
        
        # Verify it processed tables without error and logged the error
        assert len(result["T1"]) == 1  # users (5.0)
        assert len(result["T2"]) == 1  # customers (25.0)


# Arquivo: utils.py
# utils.py
import os
import logging
import configparser
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from pathlib import Path
from sql_metadata import Parser

# ConfiguraÃ§Ã£o bÃ¡sica de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SQLParser:
    """ResponsÃ¡vel por analisar e extrair informaÃ§Ãµes de queries SQL"""
    
    @staticmethod
    def extract_tables(sql_text: str) -> List[str]:
        """
        Extrai nomes de tabelas de uma query SQL.
        
        Args:
            sql_text: Texto completo da query SQL
            
        Returns:
            Lista de nomes de tabelas encontradas
        """
        try:
            tables = Parser(sql_text).tables
            logger.debug(f"Tabelas extraÃ­das: {tables}")
            return tables
        except Exception as e:
            logger.warning(f"Erro ao extrair tabelas: {e}")
            return []

    @staticmethod
    def extract_where_conditions(sql_text: str) -> List[str]:
        """
        Extrai condiÃ§Ãµes WHERE de uma query SQL de forma robusta.
        
        Args:
            sql_text: Texto completo da query SQL
            
        Returns:
            Lista de condiÃ§Ãµes WHERE encontradas
        """
        try:
            if " WHERE " not in sql_text.upper():
                return []
                
            # Extrai a parte apÃ³s WHERE mantendo o case original
            where_part = sql_text.split(" WHERE ")[1].split(";")[0]
            
            # SimplificaÃ§Ã£o para casos bÃ¡sicos
            conditions = []
            for condition in where_part.split(" AND "):
                condition = condition.strip()
                if condition:
                    # Remove sub-expressÃµes OR para simplificar
                    main_condition = condition.split(" OR ")[0].strip()
                    # Remove possÃ­veis parÃªnteses
                    main_condition = main_condition.replace("(", "").replace(")", "")
                    if main_condition:
                        conditions.append(main_condition)
            
            logger.debug(f"CondiÃ§Ãµes WHERE extraÃ­das: {conditions}")
            return conditions
            
        except Exception as e:
            logger.warning(f"Erro ao extrair condiÃ§Ãµes WHERE: {e}")
            return []


class ExecutionController:
    """Controla o estado e agendamento das execuÃ§Ãµes do monitor"""
    
    def __init__(self, config_path: str = "config/execution.ini"):
        """
        Inicializa o controlador de execuÃ§Ã£o.
        
        Args:
            config_path: Caminho para o arquivo de configuraÃ§Ã£o
        """
        self.config_path = Path(config_path)
        self.config = configparser.ConfigParser()
        self._ensure_config_dir()
        
        # Carrega configuraÃ§Ã£o se o arquivo existir
        if self.config_path.exists():
            self.config.read(self.config_path)

    def _ensure_config_dir(self) -> None:
        """Garante que o diretÃ³rio de configuraÃ§Ã£o existe"""
        self.config_path.parent.mkdir(parents=True, exist_ok=True)

    def check_first_run(self) -> bool:
        """
        Verifica se Ã© a primeira execuÃ§Ã£o do sistema.
        Retorna True apenas na primeira execuÃ§Ã£o real.
        """
        # Se o arquivo nÃ£o existe, Ã© primeira execuÃ§Ã£o
        if not self.config_path.exists():
            self._init_config_file()
            return True
            
        # Se existe mas nÃ£o tem a seÃ§Ã£o Execution
        if not self.config.has_section("Execution"):
            self._init_config_file()
            return True
            
        # Verifica o flag FIRST_RUN
        first_run = self.config.get("Execution", "FIRST_RUN", fallback="1") == "1"
        
        # Se for primeira execuÃ§Ã£o, atualiza o arquivo
        if first_run:
            self._update_config_first_run()
            
        return first_run

    def _init_config_file(self) -> None:
        """Inicializa o arquivo de configuraÃ§Ã£o para primeira execuÃ§Ã£o"""
        self.config["Execution"] = {
            "FIRST_RUN": "0",  # JÃ¡ marca como nÃ£o Ã© mais primeira execuÃ§Ã£o
            "LAST_RUN_TIMESTAMP": datetime.now().isoformat(),
            "NEXT_RUN_TIMESTAMP": ""
        }
        self._save_config()
        logger.info("Arquivo de configuraÃ§Ã£o inicializado")

    def _update_config_first_run(self) -> None:
        """Atualiza o status apÃ³s a primeira execuÃ§Ã£o"""
        self.config.set("Execution", "FIRST_RUN", "0")
        self.config.set("Execution", "LAST_RUN_TIMESTAMP", datetime.now().isoformat())
        self._save_config()
        logger.info("ConfiguraÃ§Ã£o de primeira execuÃ§Ã£o atualizada")

    def schedule_next_run(self, hours: int = 6) -> str:
        """
        Agenda a prÃ³xima execuÃ§Ã£o do monitor.
        
        Args:
            hours: Horas atÃ© a prÃ³xima execuÃ§Ã£o
            
        Returns:
            Timestamp da prÃ³xima execuÃ§Ã£o formatada
        """
        next_run = datetime.now() + timedelta(hours=hours)
        next_run_str = next_run.strftime("%Y-%m-%d %H:%M")
        
        self.config.read(self.config_path)
        self.config.set("Execution", "NEXT_RUN_TIMESTAMP", next_run_str)
        self._save_config()
        
        logger.info(f"PrÃ³xima execuÃ§Ã£o agendada para {next_run_str}")
        return next_run_str

    def _save_config(self) -> None:
        """Salva o arquivo de configuraÃ§Ã£o"""
        with open(self.config_path, "w") as config_file:
            self.config.write(config_file)


class OracleUtils:
    """UtilitÃ¡rios especÃ­ficos para Oracle Database"""
    
    @staticmethod
    def format_sql_query(query: str, params: Dict[str, Any] = None) -> str:
        """
        Formata uma query SQL com parÃ¢metros para logging.
        
        Args:
            query: Query SQL
            params: DicionÃ¡rio de parÃ¢metros
            
        Returns:
            Query formatada como string
        """
        if not params:
            return query
            
        formatted = query
        for key, value in params.items():
            formatted = formatted.replace(f":{key}", str(value))
        return formatted


def setup_logging(log_dir: str = "logs", log_file: str = "monitor.log", level: int = logging.INFO) -> None:
    """
    Configura o sistema de logging centralizado.
    
    Args:
        log_dir: DiretÃ³rio para armazenar logs
        log_file: Nome do arquivo de log
        level: NÃ­vel de logging (default: INFO)
    """
    log_path = Path(log_dir) / log_file
    log_path.parent.mkdir(parents=True, exist_ok=True)
    
    handlers = [
        logging.StreamHandler(),
        logging.FileHandler(log_path)
    ]
    
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=handlers
    )
    logger.info(f"Logging configurado. Arquivo: {log_path}")
# Teste: test_utils.py

# test_utils.py
import pytest
import configparser
from pathlib import Path
from functions.utils import ExecutionController, SQLParser, OracleUtils

class TestSQLParser:
    def test_extract_tables(self):
        assert SQLParser.extract_tables("SELECT * FROM users") == ["users"]
        assert SQLParser.extract_tables("SELECT * FROM users u JOIN orders o ON u.id=o.user_id") == ["users", "orders"]
    
    def test_extract_where_conditions(self):
        # Teste com diferentes formatos de WHERE
        conditions = SQLParser.extract_where_conditions(
            "SELECT * FROM users WHERE id=1 AND name='test'"
        )
        assert len(conditions) == 2
        assert "id=1" in conditions[0] or "id=1" in conditions[1]
        assert "name='test'" in conditions[0] or "name='test'" in conditions[1]

class TestExecutionController:
    def test_config_file_creation(self, tmp_path):
        config_file = tmp_path / "new_config.ini"
        ec = ExecutionController(config_file)
        
        # Verifica comportamento de primeira execuÃ§Ã£o
        assert ec.check_first_run() is True
        assert config_file.exists()
        
        # Verifica conteÃºdo do arquivo
        config = configparser.ConfigParser()
        config.read(config_file)
        assert config.get("Execution", "FIRST_RUN", fallback="1") == "0"
    
    def test_subsequent_runs(self, tmp_path):
        config_file = tmp_path / "existing_config.ini"
        
        # ConfiguraÃ§Ã£o inicial
        config = configparser.ConfigParser()
        config["Execution"] = {"FIRST_RUN": "0", "LAST_RUN": "2023-01-01"}
        with open(config_file, 'w') as f:
            config.write(f)
        
        # Testa com arquivo existente
        ec = ExecutionController(config_file)
        assert ec.check_first_run() is False

class TestOracleUtils:
    def test_format_sql_query(self):
        sql = "SELECT * FROM users WHERE id=:id AND date=:date"
        params = {"id": 1, "date": "2023-01-01"}
        formatted = OracleUtils.format_sql_query(sql, params)
        assert "id=1" in formatted
        assert "date=2023-01-01" in formatted
        assert "WHERE" in formatted


# Arquivo: db_utils.py
#db_utils.py 
import cx_Oracle
import logging
import os
from functools import wraps
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv
from pathlib import Path

logger = logging.getLogger(__name__)

def handle_db_errors(func):
    """
    Decorator para tratamento padrÃ£o de erros de banco de dados.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except cx_Oracle.DatabaseError as e:
            error = f"Erro de banco de dados em {func.__name__}: {e}"
            logger.error(error)
            if args and hasattr(args[0], '_last_error'):
                args[0]._last_error = error
            return None
        except Exception as e:
            error = f"Erro inesperado em {func.__name__}: {e}"
            logger.error(error)
            if args and hasattr(args[0], '_last_error'):
                args[0]._last_error = error
            return None
    return wrapper


class OracleTableReader:
    """
    Classe central para operaÃ§Ãµes de banco de dados Oracle.
    Gerencia conexÃµes, executa queries e fornece utilitÃ¡rios.
    """
    
    def __init__(self, env_file: str = None):
        """
        Inicializa o leitor de banco de dados.
        
        Args:
            env_file: Caminho opcional para arquivo .env personalizado
        """
        self._connection = None
        self._last_error = None
        self._load_config(env_file)
        
    def _load_config(self, env_file: str = None) -> None:
        """Carrega configuraÃ§Ãµes do .env"""
        env_path = env_file or os.path.join(os.getenv('PROJECT_PATH', '.'), '.env')
        try:
            load_dotenv(env_path)
            self.db_config = {
                'host': os.getenv('DB_HOST'),
                'port': os.getenv('DB_PORT'),
                'service_name': os.getenv('DB_SERVICE_NAME'),
                'username': os.getenv('DB_USERNAME'),
                'password': os.getenv('DB_PASSWORD')
            }
            logger.debug("ConfiguraÃ§Ãµes de banco de dados carregadas")
        except Exception as e:
            logger.error(f"Erro ao carregar configuraÃ§Ãµes: {e}")
            raise
            
    def __enter__(self):
        """Suporte para context manager"""
        self.connect()
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Garante que a conexÃ£o serÃ¡ fechada"""
        self.close()
        
    @property
    def last_error(self) -> Optional[str]:
        """Retorna o Ãºltimo erro ocorrido"""
        return self._last_error
    
    def connect(self) -> bool:
        """
        Estabelece conexÃ£o com o banco de dados Oracle.
        
        Returns:
            bool: True se a conexÃ£o foi bem sucedida
        """
        if self.is_connected():
            return True
            
        try:
            dsn = cx_Oracle.makedsn(
                self.db_config['host'],
                self.db_config['port'],
                service_name=self.db_config['service_name']
            )
            self._connection = cx_Oracle.connect(
                user=self.db_config['username'],
                password=self.db_config['password'],
                dsn=dsn
            )
            logger.info("âœ… ConexÃ£o com Oracle estabelecida com sucesso.")
            return True
        except cx_Oracle.DatabaseError as e:
            self._last_error = str(e)
            logger.error(f"âŒ Falha na conexÃ£o com Oracle: {e}")
            return False
        except Exception as e:
            self._last_error = str(e)
            logger.error(f"âŒ Erro inesperado ao conectar: {e}")
            return False
    
    def close(self) -> None:
        """Fecha a conexÃ£o com o banco de dados"""
        if self._connection:
            self._connection.close()
            self._connection = None
            logger.info("ðŸ”Œ ConexÃ£o com Oracle encerrada.")
    
    def is_connected(self) -> bool:
        """Verifica se hÃ¡ uma conexÃ£o ativa"""
        return self._connection is not None
    
    @handle_db_errors
    def execute_query(self, query: str, params: Optional[dict] = None, 
                    fetch_all: bool = True) -> Optional[Union[List[tuple], tuple]]:
        """
        Executa uma query SQL e retorna os resultados.
        
        Args:
            query: Query SQL a ser executada
            params: ParÃ¢metros para a query (opcional)
            fetch_all: Se True retorna todos os resultados, senÃ£o apenas um
            
        Returns:
            Resultados da query ou None em caso de erro
        """
        if not self.is_connected() and not self.connect():
            return None
                
        with self._connection.cursor() as cursor:
            cursor.execute(query, params or {})
            return cursor.fetchall() if fetch_all else cursor.fetchone()
    
    @handle_db_errors
    def get_table_size(self, table_name: str) -> Optional[float]:
        """
        ObtÃ©m o tamanho de uma tabela em MB.
        
        Args:
            table_name: Nome da tabela
            
        Returns:
            Tamanho em MB ou None se nÃ£o encontrado
        """
        query = """
            SELECT bytes / 1024 / 1024 AS size_mb
            FROM dba_segments
            WHERE segment_name = UPPER(:table_name) AND segment_type = 'TABLE'
        """
        result = self.execute_query(query, {'table_name': table_name}, fetch_all=False)
        # ExtraÃ§Ã£o segura do valor - trata tanto resultados vazios quanto estrutura de tupla
        if not result:
            return None
        return result[0] if isinstance(result, (tuple, list)) else result
    
    @handle_db_errors
    def get_table_schema(self, table_name: str) -> Optional[str]:
        """
        ObtÃ©m o schema (owner) de uma tabela.
        
        Args:
            table_name: Nome da tabela
            
        Returns:
            Nome do schema ou None se nÃ£o encontrado
        """
        query = "SELECT owner FROM all_tables WHERE table_name = UPPER(:table_name)"
        result = self.execute_query(query, {'table_name': table_name}, fetch_all=False)
        return result[0] if result else None
    
    @handle_db_errors
    def get_existing_indexes(self, table_name: str) -> List[str]:
        """
        Lista os Ã­ndices existentes para uma tabela.
        
        Args:
            table_name: Nome da tabela
            
        Returns:
            Lista de nomes de Ã­ndices
        """
        query = """
            SELECT index_name 
            FROM all_indexes 
            WHERE table_name = UPPER(:table_name)
        """
        results = self.execute_query(query, {'table_name': table_name})
        return [row[0] for row in results] if results else []

def handle_db_errors(func):
    """
    Decorator para tratamento padrÃ£o de erros de banco de dados.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except cx_Oracle.DatabaseError as e:
            error = f"Erro de banco de dados em {func.__name__}: {e}"
            logger.error(error)
            if args and hasattr(args[0], '_last_error'):
                args[0]._last_error = error
            return None
        except Exception as e:
            error = f"Erro inesperado em {func.__name__}: {e}"
            logger.error(error)
            if args and hasattr(args[0], '_last_error'):
                args[0]._last_error = error
            return None
    return wrapper
# Teste: test_db_utils.py

# test_db_utils.py
import pytest
from unittest.mock import MagicMock, patch
from functions.db_utils import OracleTableReader

class TestOracleTableReader:
    @patch('cx_Oracle.connect')
    def test_connection(self, mock_connect):
        mock_connect.return_value = MagicMock()
        reader = OracleTableReader()
        assert reader.connect() is True
    
    def test_execute_query(self):
        reader = OracleTableReader()
        mock_cursor = MagicMock()
        mock_cursor.fetchall.return_value = [("test",)]
        
        reader._connection = MagicMock()
        reader._connection.cursor.return_value.__enter__.return_value = mock_cursor
        
        result = reader.execute_query("SELECT 1 FROM DUAL")
        assert result == [("test",)]
    
    @patch('functions.db_utils.OracleTableReader.execute_query')
    def test_get_table_size(self, mock_execute):
        # Configura o mock para retornar o valor diretamente (nÃ£o uma tupla)
        mock_execute.return_value = 10.5  # Retorno direto do valor float
        
        reader = OracleTableReader()
        size = reader.get_table_size("users")
        
        assert size == 10.5
        assert isinstance(size, float)

    @patch('functions.db_utils.OracleTableReader.execute_query')
    def test_get_table_size_with_tuple(self, mock_execute):
        # Teste alternativo para quando o Oracle retorna uma tupla
        mock_execute.return_value = (10.5,)  # Formato de tupla
        
        reader = OracleTableReader()
        size = reader.get_table_size("users")
        
        assert size == 10.5
        assert isinstance(size, float)

    @patch('functions.db_utils.OracleTableReader.execute_query')
    def test_get_table_size_not_found(self, mock_execute):
        mock_execute.return_value = None  # Simula tabela nÃ£o encontrada
        reader = OracleTableReader()
        size = reader.get_table_size("nonexistent")
        assert size is None


# Arquivo: report_generator.py
# report_generation.py

from jinja2 import Environment, FileSystemLoader, TemplateNotFound
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional

def generate_html_report(
    grouped_queries: Dict,
    classified_schemas: Dict,
    output_dir: str = "output/reports",
    template_name: str = "report_template.html",
    template_dir: Optional[str] = None
) -> str:
    """
    Generate HTML report from query and schema data

    Args:
        grouped_queries: Dictionary of categorized queries
        classified_schemas: Dictionary of schema classifications
        output_dir: Output directory path
        template_name: Name of the template file
        template_dir: Optional custom template directory path

    Returns:
        Path to the generated report

    Raises:
        ValueError: If input data is invalid
        RuntimeError: If report generation fails for other reasons
    """
    # Validate input data first (before any side effects)
    if not isinstance(grouped_queries, dict) or not isinstance(classified_schemas, dict):
        raise ValueError("Input data must be dictionaries")

    try:
        # Set up template environment
        if template_dir is None:
            template_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "templates")
        
        env = Environment(loader=FileSystemLoader(template_dir))
        
        try:
            template = env.get_template(template_name)
        except TemplateNotFound:
            available = "\n".join(f" - {f}" for f in Path(template_dir).glob("*.html"))
            raise RuntimeError(
                f"Template '{template_name}' not found in {template_dir}\n"
                f"Available templates:\n{available}"
            )

        # Create output directory if it doesn't exist
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # Generate output path with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = os.path.join(output_dir, f"fts_report_{timestamp}.html")
        
        # Render and save report
        output = template.render(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M"),
            queries=grouped_queries,
            schemas=classified_schemas
        )
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(output)
            
        return report_path
        
    except Exception as e:
        raise RuntimeError(f"Failed to generate report: {str(e)}")
# Teste: test_report_generator.py

# test_report_generator.py
import pytest
from unittest.mock import patch, MagicMock
from functions.report_generator import generate_html_report
from datetime import datetime
import os
from pathlib import Path

class TestReportGeneration:
    @pytest.fixture(autouse=True)
    def setup(self, tmp_path):
        """Setup test environment with template"""
        self.template_dir = tmp_path / "templates"
        self.template_dir.mkdir()
        
        # Create test template
        template_content = """<html><body>
            <p>{{ timestamp }}</p>
            {% for cat, items in queries.items() %}
            <h2>{{ cat }}</h2>
            <ul>{% for item in items %}<li>{{ item }}</li>{% endfor %}</ul>
            {% endfor %}
            {% for cat, items in schemas.items() %}
            <h2>{{ cat }}</h2>
            <ul>{% for item in items %}<li>{{ item }}</li>{% endfor %}</ul>
            {% endfor %}
        </body></html>"""
        
        (self.template_dir / "test_template.html").write_text(template_content)
        
        self.sample_data = {
            "grouped_queries": {
                "slow": ["SELECT * FROM large_table"],
                "fast": ["SELECT id FROM users"]
            },
            "classified_schemas": {
                "T1": ["app_schema"],
                "T2": ["reporting_schema"]
            }
        }

    def test_generate_html_report_success(self, tmp_path):
        """Test successful report generation"""
        output_dir = tmp_path / "reports"
        report_path = generate_html_report(
            grouped_queries=self.sample_data["grouped_queries"],
            classified_schemas=self.sample_data["classified_schemas"],
            output_dir=str(output_dir),
            template_name="test_template.html",
            template_dir=str(self.template_dir))
        
        assert Path(report_path).exists()
        content = Path(report_path).read_text()
        assert "SELECT * FROM large_table" in content
        assert "app_schema" in content

    def test_generate_html_report_creates_directory(self, tmp_path):
        """Test that missing directories are created"""
        output_dir = tmp_path / "new" / "reports"
        report_path = generate_html_report(
            grouped_queries=self.sample_data["grouped_queries"],
            classified_schemas=self.sample_data["classified_schemas"],
            output_dir=str(output_dir),
            template_name="test_template.html",
            template_dir=str(self.template_dir))
        
        assert output_dir.exists()

    def test_generate_html_report_missing_template(self, tmp_path):
        """Test missing template handling"""
        with pytest.raises(RuntimeError, match="Template 'missing.html' not found"):
            generate_html_report(
                grouped_queries=self.sample_data["grouped_queries"],
                classified_schemas=self.sample_data["classified_schemas"],
                template_name="missing.html",
                template_dir=str(self.template_dir))

    def test_generate_html_report_invalid_data(self, tmp_path):
        """Test with invalid input data"""
        # Test invalid grouped_queries
        with pytest.raises(ValueError, match="Input data must be dictionaries"):
            generate_html_report(
                grouped_queries="invalid",
                classified_schemas={},
                template_dir=str(self.template_dir))

        # Test invalid classified_schemas
        with pytest.raises(ValueError, match="Input data must be dictionaries"):
            generate_html_report(
                grouped_queries={},
                classified_schemas="invalid",
                template_dir=str(self.template_dir))
        
    @patch('builtins.open')
    def test_generate_html_report_write_error(self, mock_open, tmp_path):
        """Test file write failure"""
        mock_open.side_effect = IOError("Write error")
        with pytest.raises(RuntimeError, match="Failed to generate report"):
            generate_html_report(
                grouped_queries=self.sample_data["grouped_queries"],
                classified_schemas=self.sample_data["classified_schemas"],
                template_dir=str(self.template_dir))


# Arquivo: script_generator.py
# script_generator.py

import os
from datetime import datetime
import logging
from typing import List, Dict, Optional

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_index_script(table_name: str, column: str = "ID") -> str:
    """Gera script para criar Ã­ndice em uma coluna especÃ­fica"""
    if not isinstance(table_name, str) or not table_name.strip():
        raise ValueError("Nome da tabela invÃ¡lido")
    if not isinstance(column, str) or not column.strip():
        raise ValueError("Nome da coluna invÃ¡lido")
        
    return f"""-- Script gerado em {datetime.now().strftime('%Y-%m-%d %H:%M')}
CREATE INDEX idx_{table_name.lower()}_{column.lower()} ON {table_name}({column});
"""

def generate_refactor_script(query: Dict) -> str:
    """Sugere refatoraÃ§Ã£o bÃ¡sica da query com dica de otimizaÃ§Ã£o"""
    required_fields = ['sql_id', 'sample_sql']
    if not all(field in query for field in required_fields):
        raise ValueError(f"Query deve conter os campos: {required_fields}")
    
    return f"""-- RefatoraÃ§Ã£o sugerida para: {query['sql_id']}
-- Tempo mÃ©dio antes: {query.get('avg_time', 'desconhecido')}ms
-- Esquema: {query.get('schema', 'desconhecido')}
{query['sample_sql']};
-- Sugerimos revisÃ£o do plano de execuÃ§Ã£o e possÃ­veis filtros adicionais.
"""

def save_sql_script(script: str, filename: str) -> bool:
    """Salva o script no diretÃ³rio de saÃ­da"""
    if not script or not isinstance(script, str):
        raise ValueError("Script invÃ¡lido")
    if not filename or not isinstance(filename, str):
        raise ValueError("Nome de arquivo invÃ¡lido")
    
    output_dir = "output/generated_scripts"
    os.makedirs(output_dir, exist_ok=True)
    file_path = os.path.join(output_dir, f"{filename}.sql")
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(script)
        logger.info(f"ðŸ“ Script salvo: {file_path}")
        return True
    except Exception as e:
        logger.error(f"[ERRO] Ao salvar script {filename}: {e}")
        return False

def generate_scripts(solutions: List[Dict]) -> List[str]:
    """
    Recebe uma lista de soluÃ§Ãµes e gera os scripts adequados.
    
    Args:
        solutions: Lista de dicionÃ¡rios com informaÃ§Ãµes de melhoria
    
    Returns:
        Lista de caminhos dos scripts gerados
    
    Raises:
        ValueError: Se solutions nÃ£o for uma lista
    """
    if not isinstance(solutions, list):
        raise ValueError("solutions deve ser uma lista")
    
    generated_files = []

    if not solutions:
        logger.warning("âš ï¸ Nenhuma soluÃ§Ã£o encontrada. Nenhum script serÃ¡ gerado.")
        return []

    for solution in solutions:
        if not isinstance(solution, dict):
            logger.warning("âš ï¸ SoluÃ§Ã£o invÃ¡lida (nÃ£o Ã© dicionÃ¡rio), ignorando")
            continue

        try:
            table_name = solution.get("table", "unknown_table")
            category = solution.get("category", "T1")
            sample_sql = solution.get("sample_sql", "")

            if category == "T1":
                script = generate_index_script(table_name)
                filename = f"T1_idx_{table_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            elif category == "T2":
                script = generate_refactor_script({
                    "sql_id": solution.get("group_key", "unknown"),
                    "avg_time": solution.get("avg_exec_time", 0),
                    "schema": solution.get("schema", "unknown"),
                    "sample_sql": sample_sql
                })
                filename = f"T2_refactor_{table_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            else:
                logger.warning(f"âš ï¸ Categoria desconhecida para tabela {table_name}: {category}")
                continue

            if save_sql_script(script, filename):
                generated_files.append(filename)
        except Exception as e:
            logger.error(f"Erro ao gerar script para {table_name}: {e}")

    logger.info(f"âœ… {len(generated_files)} scripts SQL gerados.")
    return generated_files
# Teste: test_script_generator.py

# test_script_generator.py
import pytest
from unittest.mock import patch, mock_open, MagicMock
from functions.script_generator import (
    generate_scripts,
    generate_index_script,
    generate_refactor_script,
    save_sql_script
)
from datetime import datetime
import logging

class TestScriptGenerator:
    def test_generate_index_script_success(self):
        """Test successful index script generation"""
        result = generate_index_script("Users", "ID")
        assert "CREATE INDEX idx_users_id ON Users(ID)" in result
        assert datetime.now().strftime("%Y-%m-%d") in result

    def test_generate_index_script_invalid_input(self):
        """Test index script with invalid input"""
        with pytest.raises(ValueError):
            generate_index_script("", "id")
        with pytest.raises(ValueError):
            generate_index_script("users", "")

    def test_generate_refactor_script_success(self):
        """Test successful refactor script generation"""
        test_data = {
            "sql_id": "q123",
            "avg_time": 150,
            "schema": "public",
            "sample_sql": "SELECT * FROM users"
        }
        result = generate_refactor_script(test_data)
        assert "q123" in result
        assert "150ms" in result
        assert "public" in result
        assert "SELECT * FROM users" in result

    def test_generate_refactor_script_missing_fields(self):
        """Test refactor script with missing required fields"""
        with pytest.raises(ValueError):
            generate_refactor_script({"sql_id": "q123"})  # missing sample_sql

    @patch("os.makedirs")
    @patch("builtins.open", new_callable=mock_open)
    def test_save_sql_script_success(self, mock_file, mock_makedirs):
        """Test successful script saving"""
        assert save_sql_script("TEST SCRIPT", "test") is True
        mock_makedirs.assert_called_once()
        mock_file.assert_called_once()

    @patch("os.makedirs")
    @patch("builtins.open", side_effect=IOError("Test error"))
    def test_save_sql_script_failure(self, mock_file, mock_makedirs):
        """Test script saving failure"""
        assert save_sql_script("TEST SCRIPT", "test") is False

    def test_save_sql_script_invalid_input(self):
        """Test script saving with invalid input"""
        with pytest.raises(ValueError):
            save_sql_script("", "test")
        with pytest.raises(ValueError):
            save_sql_script("VALID", "")

    def test_generate_scripts_empty(self):
        """Test with empty solutions list"""
        assert generate_scripts([]) == []

    def test_generate_scripts_invalid_input(self):
        """Test with invalid input type"""
        with pytest.raises(ValueError):
            generate_scripts("not a list")

    @patch("functions.script_generator.save_sql_script", return_value=True)
    def test_generate_scripts_t1(self, mock_save):
        """Test T1 script generation"""
        solutions = [{
            "table": "users",
            "category": "T1",
            "priority_score": 5
        }]
        result = generate_scripts(solutions)
        assert len(result) == 1
        assert "T1_idx_users" in result[0]

    @patch("functions.script_generator.save_sql_script", return_value=True)
    def test_generate_scripts_t2(self, mock_save):
        """Test T2 script generation"""
        solutions = [{
            "table": "orders",
            "category": "T2",
            "group_key": "q123",
            "sample_sql": "SELECT * FROM orders",
            "avg_exec_time": 200
        }]
        result = generate_scripts(solutions)
        assert len(result) == 1
        assert "T2_refactor_orders" in result[0]

    def test_generate_scripts_invalid_item(self, caplog):
        """Test with invalid solution item"""
        result = generate_scripts(["not a dict"])
        assert len(result) == 0
        assert "SoluÃ§Ã£o invÃ¡lida" in caplog.text

    def test_generate_scripts_unknown_category(self, caplog):
        """Test with unknown category"""
        result = generate_scripts([{
            "table": "logs",
            "category": "T3"
        }])
        assert len(result) == 0
        assert "Categoria desconhecida" in caplog.text

    @patch("functions.script_generator.generate_index_script", side_effect=Exception("Test error"))
    def test_generate_scripts_handles_errors(self, mock_gen, caplog):
        """Test error handling during generation"""
        result = generate_scripts([{
            "table": "users",
            "category": "T1"
        }])
        assert len(result) == 0
        assert "Erro ao gerar script" in caplog.text


# Arquivo: performance_improvement.py

## performance_improvement.py
import logging
from typing import List, Dict, Any, Optional
from functions.db_utils import OracleTableReader

logger = logging.getLogger(__name__)

def evaluate_performance(db_reader: OracleTableReader, 
                        tables: Dict[str, List[Dict[str, Any]]],
                        queries: Optional[List[Dict[str, Any]]] = None) -> List[Dict[str, Any]]:
    """
    Avalia impacto potencial de melhorias usando OracleTableReader.
    
    Args:
        db_reader: InstÃ¢ncia de OracleTableReader
        tables: DicionÃ¡rio com tabelas classificadas (T1 e T2)
        queries: Lista opcional de queries para anÃ¡lise
        
    Returns:
        Lista de dicionÃ¡rios com sugestÃµes de melhoria
    
    Raises:
        TypeError: Se os parÃ¢metros forem de tipos invÃ¡lidos
    """
    # ValidaÃ§Ã£o de entrada
    if not isinstance(db_reader, OracleTableReader):
        logger.error("âŒ db_reader deve ser uma instÃ¢ncia de OracleTableReader")
        raise TypeError("db_reader deve ser uma instÃ¢ncia de OracleTableReader")
        
    if not isinstance(tables, dict):
        logger.error("âŒ tables deve ser um dicionÃ¡rio")
        raise TypeError("tables deve ser um dicionÃ¡rio")
    
    if not db_reader.is_connected():
        logger.error("âŒ ConexÃ£o com banco de dados nÃ£o disponÃ­vel.")
        return []
    
    solutions = []
    
    try:
        # Processar tabelas T1
        for table_info in tables.get("T1", []):
            if not isinstance(table_info, dict):
                logger.warning(f"âš ï¸ InformaÃ§Ãµes invÃ¡lidas para tabela T1: {table_info}")
                continue
                
            table_name = table_info.get("table")
            if not table_name:
                logger.warning("âš ï¸ Tabela T1 sem nome, ignorando")
                continue
                
            indexes = db_reader.get_existing_indexes(table_name)
            
            solutions.append({
                "table": table_name,
                "size_mb": table_info.get("size_mb", 0),
                "category": "T1",
                "suggestion": "criar Ã­ndice" if not indexes else "Ã­ndice jÃ¡ existe",
                "existing_indexes": indexes,
                "priority_score": 8 if not indexes else 2,
                "impact": "Alta" if not indexes else "Baixa"
            })
        
        # Processar tabelas T2
        for table_info in tables.get("T2", []):
            if not isinstance(table_info, dict):
                logger.warning(f"âš ï¸ InformaÃ§Ãµes invÃ¡lidas para tabela T2: {table_info}")
                continue
                
            table_name = table_info.get("table")
            if not table_name:
                logger.warning("âš ï¸ Tabela T2 sem nome, ignorando")
                continue
                
            size_mb = table_info.get("size_mb", 0)
            indexes = db_reader.get_existing_indexes(table_name)
            
            relevant_queries = [
                q for q in (queries or [])
                if isinstance(q, dict) and table_name in q.get("tables", [])
            ]
            
            gain = sum(
                _estimate_gain(q, size_mb)["estimated_gain_percent"]
                for q in relevant_queries
            ) / len(relevant_queries) if relevant_queries else 0
            
            solutions.append({
                "table": table_name,
                "size_mb": size_mb,
                "category": "T2",
                "suggestion": "refatorar query ou particionar" if indexes else "considerar Ã­ndice",
                "queries_affected": [q.get("sql_id") for q in relevant_queries if q.get("sql_id")],
                "avg_gain_percent": round(gain, 2),
                "existing_indexes": indexes,
                "priority_score": 9 if gain > 20 else (7 if gain > 0 else 5),
                "impact": "CrÃ­tico" if gain > 20 else ("MÃ©dio" if gain > 0 else "Baixo")
            })
        
        logger.info(f"ðŸ’¡ {len(solutions)} sugestÃµes de melhoria geradas.")
        return solutions
        
    except Exception as e:
        logger.error(f"âŒ Erro ao avaliar performance: {str(e)}")
        raise RuntimeError(f"Erro ao avaliar performance: {str(e)}")

def _estimate_gain(query: Dict[str, Any], table_size_mb: float) -> Dict[str, Any]:
    """Estimativa de ganho de performance para uma query."""
    if not isinstance(query, dict):
        raise ValueError("Query deve ser um dicionÃ¡rio")
        
    if not isinstance(table_size_mb, (int, float)) or table_size_mb < 0:
        raise ValueError("table_size_mb deve ser um nÃºmero positivo")
    
    base_time = max(1, query.get("elapsed_time", 100))
    gain_factor = 0.6 if table_size_mb < 10 else (0.4 if table_size_mb < 50 else 0.2)
    
    return {
        "estimated_gain_percent": round(gain_factor * 100, 2),
        "old_time": base_time,
        "new_time": round(base_time * (1 - gain_factor), 2),
        "schema": query.get("schema"),
        "sql_id": query.get("sql_id")
    }
# Teste: test_performance_improvement.py

# test_performance_improvement.py
import pytest
from unittest.mock import patch, MagicMock
from functions.performance_improvement import (
    evaluate_performance,
    _estimate_gain
)
from functions.db_utils import OracleTableReader

class TestPerformanceImprovement:
    @pytest.fixture
    def mock_reader(self):
        mock = MagicMock(spec=OracleTableReader)
        mock.is_connected.return_value = True
        return mock

    @pytest.fixture
    def sample_tables(self):
        return {
            "T1": [{"table": "users", "size_mb": 5.0}],
            "T2": [{"table": "orders", "size_mb": 15.0}]
        }

    @pytest.fixture
    def sample_queries(self):
        return [
            {"sql_id": "q1", "tables": ["users"], "elapsed_time": 100},
            {"sql_id": "q2", "tables": ["orders"], "elapsed_time": 200}
        ]

    def test_evaluate_performance_basic(self, mock_reader, sample_tables):
        """Test basic performance evaluation"""
        mock_reader.get_existing_indexes.return_value = []
        solutions = evaluate_performance(mock_reader, sample_tables)
        
        assert len(solutions) == 2
        assert solutions[0]["suggestion"] == "criar Ã­ndice"
        assert solutions[1]["suggestion"] == "considerar Ã­ndice"

    def test_evaluate_performance_with_queries(self, mock_reader, sample_tables, sample_queries):
        """Test evaluation with query analysis"""
        mock_reader.get_existing_indexes.return_value = ["existing_idx"]
        solutions = evaluate_performance(mock_reader, sample_tables, sample_queries)
        
        assert len(solutions) == 2
        assert solutions[1]["queries_affected"] == ["q2"]
        assert isinstance(solutions[1]["avg_gain_percent"], float)

    def test_evaluate_performance_no_connection(self, mock_reader, sample_tables):
        """Test with no database connection"""
        mock_reader.is_connected.return_value = False
        solutions = evaluate_performance(mock_reader, sample_tables)
        assert solutions == []

    def test_evaluate_performance_invalid_input(self, mock_reader):
        """Test with invalid input types"""
        with pytest.raises(TypeError):
            evaluate_performance(None, {})
            
        with pytest.raises(TypeError):
            evaluate_performance(mock_reader, "not a dict")

    def test_evaluate_performance_empty_tables(self, mock_reader):
        """Test with empty tables dictionary"""
        solutions = evaluate_performance(mock_reader, {})
        assert solutions == []

    def test_evaluate_performance_invalid_table_data(self, mock_reader, caplog):
        """Test with invalid table data"""
        solutions = evaluate_performance(mock_reader, {"T1": ["invalid"]})
        assert len(solutions) == 0
        assert "InformaÃ§Ãµes invÃ¡lidas" in caplog.text

    def test_estimate_gain_basic(self):
        """Test basic gain estimation"""
        result = _estimate_gain({"elapsed_time": 100}, 8.0)
        assert result["estimated_gain_percent"] > 0
        assert result["new_time"] < 100

    def test_estimate_gain_various_sizes(self):
        """Test gain estimation with different table sizes"""
        small = _estimate_gain({"elapsed_time": 100}, 5.0)
        medium = _estimate_gain({"elapsed_time": 100}, 30.0)
        large = _estimate_gain({"elapsed_time": 100}, 100.0)
        
        assert small["estimated_gain_percent"] > medium["estimated_gain_percent"]
        assert medium["estimated_gain_percent"] > large["estimated_gain_percent"]

    def test_estimate_gain_invalid_input(self):
        """Test gain estimation with invalid input"""
        with pytest.raises(ValueError):
            _estimate_gain("not a dict", 10.0)
            
        with pytest.raises(ValueError):
            _estimate_gain({}, -1.0)

    @patch('functions.performance_improvement.logger')
    def test_error_handling(self, mock_logger, mock_reader):
        """Test error handling during evaluation"""
        mock_reader.is_connected.side_effect = Exception("DB error")
        with pytest.raises(RuntimeError):
            evaluate_performance(mock_reader, {"T1": [{"table": "users"}]})
        mock_logger.error.assert_called()


